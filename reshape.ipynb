{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='config.cfg', imports=[], includes=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from DataLoader import (\n",
    "    AfricanWildlifeDataset,\n",
    "    show_image,\n",
    "    show_pair,\n",
    "    show_triple,\n",
    "    gaussian_noise_transform,\n",
    ")\n",
    "from DenoisingAE import DenoisingAE, DenoisingAEV1\n",
    "import torch\n",
    "import gc\n",
    "import torchvision\n",
    "import yaml\n",
    "import gin\n",
    "import os\n",
    "from itertools import product\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.image\n",
    "import cv2\n",
    "\n",
    "best_checkpoint_path_20_v1 = \"/home/edoardo/aiproject/tb_logs/denoising_autoencoder_v1/20/checkpoints/epoch=142-step=18876.ckpt\"\n",
    "best_checkpoint_path_10_v1 = \"/home/edoardo/aiproject/tb_logs/denoising_autoencoder_v1/10/checkpoints/epoch=70-step=9372.ckpt\"\n",
    "best_checkpoint_path_20_v0 = \"/home/edoardo/aiproject/tb_logs/denoising_autoencoder_v0/20/checkpoints/epoch=237-step=31416.ckpt\"\n",
    "last_checkpoint_path_10_v0 = \"/home/edoardo/aiproject/tb_logs/denoising_autoencoder_v0/10_1/checkpoints/epoch=237-step=31416.ckpt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "gin.parse_config_file(\"config.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '../datasets/wildlife', 'train': 'train/images', 'val': 'valid/images', 'test': 'test/images', 'names': {0: 'buffalo', 1: 'elephant', 2: 'rhino', 3: 'zebra'}}\n",
      "{'path': '../datasets/wildlife', 'train': 'train/images', 'val': 'valid/images', 'test': 'test/images', 'names': {0: 'buffalo', 1: 'elephant', 2: 'rhino', 3: 'zebra'}, 'datasets_dir': '/home/edoardo/aiproject/aaa'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "y = None\n",
    "with open(f\"./wildlife.yaml\") as stream:\n",
    "    this_path = os.path.join(os.getcwd(), \"aaa\")\n",
    "    try:\n",
    "        y = yaml.safe_load(stream)\n",
    "        print(y)\n",
    "        y[\"datasets_dir\"] = this_path\n",
    "        print(y)\n",
    "        # yaml.safe_dump(y, stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# with open(f\"{Path.home()}/.config/Ultralytics/settings.yaml\", \"w\") as f:\n",
    "#     yaml.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edoardo/anaconda3/envs/aiproject/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "def copy_images(dataset, model, noise):\n",
    "    for index in range(len(dataset)):\n",
    "        img, orig = dataset[index]\n",
    "        shape = dataset.image_dimensions[index]\n",
    "        original_name = dataset.list_dir[index]\n",
    "\n",
    "        t = torchvision.transforms.Resize((shape[1], shape[2]))\n",
    "\n",
    "        # Clean and returning to the original size\n",
    "        cleanedv0 = t(model(img.to(\"cuda\")).detach().cpu())\n",
    "        # Transpose the axes to make it WHC and reconvert it to a [0,...,255] image\n",
    "        normalized = np.transpose((cleanedv0 * 255).numpy(), [1, 2, 0]).astype(\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        matplotlib.image.imsave(\n",
    "            f\"datasets/wildlife_{noise}_{model.kind}/{dataset.kind}/images/{original_name}\",\n",
    "            normalized,\n",
    "        )\n",
    "\n",
    "\n",
    "for noise, model in product([0.1, 0.2]):\n",
    "    if noise == 0.1:\n",
    "        modelv1 = DenoisingAEV1.load_from_checkpoint(best_checkpoint_path_10_v1)\n",
    "        modelv0 = DenoisingAE.load_from_checkpoint(best_checkpoint_path_10_v0)\n",
    "    elif noise == 0.2:\n",
    "        modelv1 = DenoisingAEV1.load_from_checkpoint(best_checkpoint_path_20_v1)\n",
    "        modelv0 = DenoisingAE.load_from_checkpoint(best_checkpoint_path_20_v0)\n",
    "    for model in [modelv0, modelv1]:\n",
    "        train_dataset = AfricanWildlifeDataset(\n",
    "            kind=\"train\", transform=gaussian_noise_transform(0, noise, width=640)\n",
    "        )\n",
    "        val_dataset = AfricanWildlifeDataset(\n",
    "            kind=\"valid\", transform=gaussian_noise_transform(0, noise, width=640)\n",
    "        )\n",
    "        test_dataset = AfricanWildlifeDataset(\n",
    "            kind=\"test\", transform=gaussian_noise_transform(0, noise, width=640)\n",
    "        )\n",
    "        dataset_yaml = dict(\n",
    "            {\n",
    "                \"path\": f\"../datasets/wildlife_{noise}_{model.kind}\",\n",
    "                \"train\": \"train/images\",\n",
    "                \"val\": \"valid/images\",\n",
    "                \"test\": \"test/images\",\n",
    "                \"names\": {0: \"buffalo\", 1: \"elephant\", 2: \"rhino\", 3: \"zebra\"},\n",
    "            }\n",
    "        )\n",
    "        with open(f\"./wildlife_{noise}_{model.kind}.yaml\", \"w\") as f:\n",
    "            yaml.dump(dataset_yaml, f)\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}\")\n",
    "\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/train\")\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/train/images\")\n",
    "        # we just copy the labels because we have kept the right image dimensions\n",
    "        shutil.copytree(\n",
    "            \"datasets/wildlife/train/labels\",\n",
    "            f\"datasets/wildlife_{noise}_{model.kind}/train/labels\",\n",
    "        )\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/test\")\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/test/images\")\n",
    "        # we just copy the labels because we have kept the right image dimensions\n",
    "        shutil.copytree(\n",
    "            \"datasets/wildlife/test/labels\",\n",
    "            f\"datasets/wildlife_{noise}_{model.kind}/test/labels\",\n",
    "        )\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/valid\")\n",
    "        os.mkdir(f\"datasets/wildlife_{noise}_{model.kind}/valid/images\")\n",
    "        # we just copy the labels because we have kept the right image dimensions\n",
    "        shutil.copytree(\n",
    "            \"datasets/wildlife/valid/labels\",\n",
    "            f\"datasets/wildlife_{noise}_{model.kind}/valid/labels\",\n",
    "        )\n",
    "        copy_images(train_dataset, model, noise)\n",
    "        copy_images(test_dataset, model, noise)\n",
    "        copy_images(val_dataset, model, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
